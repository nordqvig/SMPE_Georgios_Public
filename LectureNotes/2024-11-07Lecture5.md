# Lecture 5 (7/11 2024)
See slides on: https://github.com/alegrand/SMPE/tree/master/sessions/2024_10_Grenoble: 7/11/23

## Correlation, Causation, Statistics

#### Example: Cholesterol and Statins
Study from 70s correlated heart-disease with average fat consumption in diet. Graph looked really well fit 
(high R value) except they had removed some data points.

Measure dependence between two variables using correlation measure:

corr(X, Y) = cov(X, Y) / (sigma_X * sigma_Y), where cov(X,Y) = E[(X - mu_X)(Y - mu_Y)]
corr(X, Y) $\in$ [-1, 1]
corr(X, Y) = 0 => X and Y are independent. \
Correlation was devised with linear relationship in mind. It does not model other relationships well.

#### Correlation does not imply causation
- 2 variables (Global Average Temperature & Number of Pirates) can be strongly correlated to a third one (time).
- Divorce rate in South Carolina vs Honey producing bee colonies (R = 0.904)
These relations were not hypothesized first and investigated after. Instead, they were found by looking for
strong correlations. This means the real problem is not the data/numbers, but it is the process (behind the figure).

## Why do we need to visualize ?
Vastly different datasets can give the same statistical measures (mean, correlation, slope, etc...).Visualize the 
datasets to see the data and see if there is an issue with the data.

## Variability and confidence interval
As we have more and more data points, the observed mean tends to the theoretical expected value.\
It is essential to understand how confidence intervals work and to understand them intuitively.

## Probabilities
Probability space (omega, F, P)\
Omega: The sample space (all possible combos of my DNA with my girlfriend's DNA)\
F: the event (eg. the event that you have a baby girl)\
P: The probability measure: is a function returning an event's probability P("having a brown-eyes baby girl") = 0.00005)

S_n = 1/n * ( X_1 + ... + X_n) is the estimator **Sample Mean**\
An esimator is when you build a bigger box out of several small boxes (random variables).

Confidence Interval visualized: see 2024-11-07Lecture5ConfidenceInterval.png\

**The width of the CI $\propto$ $\sigma$ / $n^{1/2}$**\
This allows you to **plan** ahead of your experiment, to gauge how much computing power (n) that you will need to 
achieve your goal CI. Using the fact that **halving the CI requires 4 times more experiments**.\
Instead, we can use blocking! Reduce variance by exploiting blocks (see the slides).

Homework: Compute CI for quicksort data

CLT hypothesis only requires i.i.d variables. Then as n grows large, the **sample mean** is normally distributed. It is 
a very strong tool. However, the CLT uses $\sigma$. But we only have the **sample variance**. This will cause issues 
with your later computations, where you are forced to use the **sample variance**. Try to bound the variance($\sigma$) 
(if the variable itself is bounded, this is possible!) and use your theoretical bound instead. This can improve your CI.

### Q: How many replicates?
### A: 30
Longer answer is to perform experiments sequentially. Start with n = 30, compute a first CI, then see whether you need 
to increase n, and if so, by how much. And keep a record of this (journal!) so that the decision-making process is 
transparent.

You can perform Simple Graphical Checks (see slides) to check your data before you start statistical calculations. The 
visualizations are important to catch issues with your data sets. Get into the habit of doing it.

### Comparing two alternatives

- Comparing A and B for different settings, don't do all A then all B. Better to randomize run order, to 
eliminate/identify the impact one instance has on the coming ones.

### Experimental Design
**Replication** and **randomization** are the key concepts. 
- Replicate to increase reliability.
- Randomize to reduce bias. 








