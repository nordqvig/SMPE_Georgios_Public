# Lecture 7 (21/11 2024) 

*Summarized learnings from last lecture (which I didn't attend): descriptive vs. inferential statistics, loss function, 
$R^{2}$ value to measure the appropriateness of an inferential model.*

A linear model fit gives you:

* Estimate: the proposed value of the parameter
* Std. Error: $\sigma/\sqrt{n}$
* t value: Probability that the parameter = 0 
* p-value of the entire model: probability that the whole model = 0

Adjusted R square: accounts for how many variables you have in the model - more variables mean smaller R-squared
(but too many could give R2 = 1 no problem) so this value takes that into account.

Homoscedastic, heteroscedastic, quadratic data sets\
There is an interplay between the intercept and slope parameters. They will not both be "small" or "large" 
at the same time.

## Extensions Linear Model

### Discrete Variables: ANOVA
Type1 errors: "seeing something that doesn't exist" (a relationship, trend, etc...) This is combatted using 
analysis of variance.

ANOVA
* Enables to prove that some parameters have little influence 
* Decomposes the variance to allow for deep analysis (see slides)

RCMDR (GUI for all the statistical tools in R, organized in menus) can be very useful!