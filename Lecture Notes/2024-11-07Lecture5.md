# Lecture 5 (7/11 2024)
See slides on: https://github.com/alegrand/SMPE/tree/master/sessions/2024_10_Grenoble: 7/11/23

## Correlation, Causation, Statistics

#### Example: Cholesterol and Statins
Study from 70s correlated heart-disease with average fat consumption in diet. Graph looked really well fit 
(high R value) except they had removed some data points.

Measure dependence between two variables using correlation measure:

corr(X, Y) = cov(X, Y) / (sigma_X * sigma_Y), where cov(X,Y) = E[(X - mu_X)(Y - mu_Y)]
corr(X, Y) $\in$ [-1, 1]
corr(X, Y) = 0 => X and Y are independent. \
Correlation was devised with linear relationship in mind. It does not model other relationships well.

#### Correlation does not imply causation
- 2 variables (Global Average Temperature & Number of Pirates) can be strongly correlated to a third one (time).
- Divorce rate in South Carolina vs Honey producing bee colonies (R = 0.904)
These relations were not hypothesized first and investigated after. Instead, they were found by looking for
strong correlations. This means the real problem is not the data/numbers, but it is the process (behind the figure).

## Why do we need to visualize ?
Vastly different datasets can give the same statistical measures (mean, correlation, slope, etc...).Visualize the 
datasets to see the data and see if there is an issue with the data.

## Variability and confidence interval
As we have more and more data points, the observed mean tends to the theoretical expected value.\
It is essential to understand how confidence intervals work and to understand them intuitively.

## Probabilities
Probability space (omega, F, P)\
Omega: The sample space (all possible combos of my DNA with my girlfriend's DNA)\
F: the event (eg. the event that you have a baby girl)\
P: The probability measure: is a function returning an event's probability P("having a brown-eyes baby girl") = 0.00005)

S_n = 1/n * ( X_1 + ... + X_n) is the esimator **Sample Mean**

Confidence Interval visualized: see 2024-11-07Lecture5ConfidenceInterval.png\

**The width of the CI $\propto$ $\sigma$ / $n^{1/2}$**\
This allows you to **plan** ahead of your experiment, to gauge how much computing power (n) that you will need to 
achieve your goal CI. Using the fact that **halving the CI requires 4 times more experiments**.\
Instead, we can use blocking! Reduce variance by exploiting blocks (see the slides).







